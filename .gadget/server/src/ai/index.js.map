{"version":3,"sources":["/app/packages/framework/src/ai/index.ts"],"sourcesContent":["import { Readable, type ReadableOptions } from \"stream\";\n\nclass OpenAIResponseStream extends Readable {\n  readonly openAIIterable: AsyncIterable<any>;\n  reading: boolean;\n  result: string;\n\n  constructor(openAIIterable: AsyncIterable<any>, options: ReadableOptions = {}) {\n    super(options);\n    this.openAIIterable = openAIIterable;\n    this.reading = false;\n    this.result = \"\";\n  }\n\n  async _read() {\n    if (this.reading) return;\n    this.reading = true;\n\n    try {\n      for await (const part of this.openAIIterable) {\n        const content = part.choices?.[0]?.delta?.content ?? part.choices?.[0]?.text;\n        if (content) {\n          this.result += content;\n          this.push(content);\n        }\n      }\n\n      this.push(null);\n      this.reading = false;\n    } catch (err) {\n      this.emit(\"error\", err);\n    }\n  }\n}\n\n/**\n * Represents options for the OpenAI response stream.\n */\nexport interface OpenAIResponseStreamOptions {\n  /**\n   * A callback function that will be invoked when the OpenAI response stream is complete.\n   * @param {string} content - The full output of the LLM response.\n   */\n  onComplete?: (content: string) => void;\n}\n\n/**\n * Converts the result of calling openai with `stream: true` into a readable stream that\n * Fasitfy can respond with.\n *\n *\n * @param {AsyncIterable<any>} stream - An AsyncIterable containing OpenAI response parts.\n * @param {OpenAIResponseStreamOptions} options - Options for the OpenAI response stream.\n * @returns {Readable} A Readable stream with the transformed content from the input stream.\n *\n *\n * @example\n * // Using the openAIResponseStream function to convert an AsyncIterable into a Readable stream\n * const stream = await connections.openai.chat.completions.create({\n *   model: \"gpt-3.5-turbo\",\n *   messages: [{ role: \"user\", content: \"Hello!\" }],\n *   stream: true,\n * });\n * await reply.send(openAIResponseStream(stream, {\n *  onComplete: (content) => { console.log(content) }\n * }));\n *\n * @see {@link https://github.com/openai/openai-node} - OpenAI Node.js client library.\n * @see {@link https://docs.gadget.dev/guides/http-routes/route-configuration#sending-responses} - Sending responses in Gadget.\n */\nexport function openAIResponseStream(openAIIterable: AsyncIterable<any>, options: OpenAIResponseStreamOptions = {}): Readable {\n  const stream = new OpenAIResponseStream(openAIIterable);\n\n  stream.on(\"end\", () => {\n    if (options.onComplete) options.onComplete(stream.result);\n  });\n\n  return stream;\n}\n"],"names":["openAIResponseStream","OpenAIResponseStream","Readable","openAIIterable","reading","result","constructor","options","_read","part","content","choices","delta","text","push","err","emit","stream","on","onComplete"],"mappings":";;;;+BAsEgBA;;aAAAA;;;yBAtE+B;;;;;;AAE/C,MAAMC,6BAA6BC,kBAAQ;IAChCC,eAAmC;IAC5CC,QAAiB;IACjBC,OAAe;IAEfC,YAAYH,cAAkC,EAAEI,UAA2B,CAAC,CAAC,CAAE;QAC7E,KAAK,CAACA;QACN,IAAI,CAACJ,cAAc,GAAGA;QACtB,IAAI,CAACC,OAAO,GAAG,KAAK;QACpB,IAAI,CAACC,MAAM,GAAG;IAChB;IAEA,MAAMG,QAAQ;QACZ,IAAI,IAAI,CAACJ,OAAO,EAAE;QAClB,IAAI,CAACA,OAAO,GAAG,IAAI;QAEnB,IAAI;YACF,WAAW,MAAMK,QAAQ,IAAI,CAACN,cAAc,CAAE;gBAC5C,MAAMO,UAAUD,KAAKE,OAAO,EAAE,CAAC,EAAE,EAAEC,OAAOF,WAAWD,KAAKE,OAAO,EAAE,CAAC,EAAE,EAAEE;gBACxE,IAAIH,SAAS;oBACX,IAAI,CAACL,MAAM,IAAIK;oBACf,IAAI,CAACI,IAAI,CAACJ;gBACZ,CAAC;YACH;YAEA,IAAI,CAACI,IAAI,CAAC,IAAI;YACd,IAAI,CAACV,OAAO,GAAG,KAAK;QACtB,EAAE,OAAOW,KAAK;YACZ,IAAI,CAACC,IAAI,CAAC,SAASD;QACrB;IACF;AACF;AAqCO,SAASf,qBAAqBG,cAAkC,EAAEI,UAAuC,CAAC,CAAC,EAAY;IAC5H,MAAMU,SAAS,IAAIhB,qBAAqBE;IAExCc,OAAOC,EAAE,CAAC,OAAO,IAAM;QACrB,IAAIX,QAAQY,UAAU,EAAEZ,QAAQY,UAAU,CAACF,OAAOZ,MAAM;IAC1D;IAEA,OAAOY;AACT"}